# Changelog

All notable changes to the Expense Predictor project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

## [1.27.0] - 2026-02-14

### Added
- **Quantile Regression Forecasting**: New probabilistic forecasting capability for budgeting scenarios with prediction intervals instead of point estimates.
  - Multi-quantile predictions (P50, P75, P90) per forecast period for expense budgeting ranges
  - Support for both Gradient Boosting and Linear quantile regression models via `quantile_forecasting.py` module
  - Pinball loss evaluation metric for quantile prediction accuracy assessment
  - Coverage metrics validation to ensure quantile predictions are well-calibrated
  - Unified CSV output format with P50, P75, P90 columns for Excel integration
  - New `quantile_forecasting` configuration section in `config.yaml` with enable/disable toggle, configurable quantiles, and model type selection
  - Pydantic validation for quantile forecasting configuration ensuring quantiles are in (0, 1) range and sorted
  - Integration into main model_runner.py pipeline with automatic training, evaluation, and prediction generation
  - Comprehensive test suite with 23 unit tests covering all quantile forecasting functionality

### Changed
- Version bump 1.26.0 → 1.27.0 (minor) for new quantile regression forecasting capability
- Enhanced model comparison metrics to include quantile-specific metrics (pinball_loss, coverage, coverage_error) in reporting

### Technical Details
- Quantile regression uses sklearn's GradientBoostingRegressor with `loss='quantile'` or QuantileRegressor for linear models
- Pinball loss (quantile loss) evaluates asymmetric prediction errors based on quantile level
- Coverage metrics measure proportion of actual values falling below predicted quantile
- Future predictions generated by retraining on full dataset after test evaluation
- CSV output includes Date column in DD/MM/YYYY format with P50, P75, P90 prediction columns
- Configuration validated via Pydantic with automatic quantile sorting for consistency

### Documentation
- Updated README with quantile forecasting feature description and configuration examples
- Added quantile_forecasting module to setup.py py_modules list for package distribution

## [1.26.0] - 2026-02-14

### Added
- Dedicated **SARIMAX** and **Prophet** forecasting pipelines integrated into `model_runner.py` with optional exogenous regressors.
- New `time_series_models` configuration section in `config.yaml`/`config.py` for enabling/disabling models, tuning key parameters, and controlling artifact persistence.
- Fitted time-series model artifact persistence in `output_dir/artifacts/` for reproducibility.
- New production dependencies: `statsmodels` and `prophet`.

### Changed
- Model comparison pipeline now includes dedicated time-series metrics in the same schema as existing ML/baseline reports.
- Extended supported production model names to include `SARIMAX` and `Prophet`.
- Version bump 1.25.0 → 1.26.0 (minor) for new forecasting model capabilities.

### Fixed
- Excluded constant-valued exogenous columns before registering Prophet regressors, preventing fit-time failures on zero-variance features.
- Updated dedicated SARIMAX and Prophet future exogenous feature generation to use recursive prediction feedback so lag/rolling regressors remain realistic across multi-day horizons.

### Documentation
- Updated README with dedicated time-series model capabilities and configuration examples.

## [1.25.0] - 2026-02-14

### Added
- **Time-Series Feature Engineering**: New `feature_engineering.py` module providing configurable time-aware features to capture spending momentum, trends, and seasonal cycles:
  - **Lag features** (t-1, t-3, t-6, t-12): Target values from N days prior to capture recent spending patterns
  - **Rolling statistics** (7, 14, 30 day windows): Rolling mean and standard deviation with shift to prevent data leakage
  - **Calendar features**: Quarter and year columns to complement existing month and day-of-month features
- **Feature Engineering Configuration**: New `feature_engineering` section in `config.yaml` with Pydantic-validated settings for lags, rolling windows, and calendar feature toggles
- **Feature List Artifact**: Feature names saved to `reports/feature_list.json` for debugging and reproducibility
- **Future Prediction Support**: Time-series features for future dates computed from historical data tail, ensuring meaningful lag/rolling values at the prediction boundary
- **NaN Handling**: Automatic removal of rows with NaN values introduced by lag/rolling windows, with aligned X, y, and processed DataFrame
- New `FeatureEngineeringConfig` Pydantic validation class in `config.py`
- 30 new unit tests in `tests/test_feature_engineering.py` covering lag features, rolling statistics, calendar features, NaN handling, feature persistence, and future feature preparation

### Changed
- `_process_dataframe()` in `helpers.py` now calls time-series feature engineering after base calendar features
- `prepare_future_dates()` in `helpers.py` accepts optional `historical_df` parameter for computing lag/rolling features on future dates
- `train_and_evaluate_models()` and `_make_future_predictions()` in `model_runner.py` pass processed historical data through the prediction pipeline
- Updated two existing integration tests to account for NaN-dropped rows from time-series feature engineering
- Version bump 1.24.0 → 1.25.0 (minor) for new time-series feature engineering capability

### Technical Details
- Feature generation is deterministic: same input always produces same output
- Rolling features use `shift(1)` before computation to prevent target leakage
- NaN rows from the largest rolling window (default 30) are dropped from the start of the dataset
- Future predictions use the historical data tail to compute lag/rolling features across the train/future boundary
- All features configurable via `config.yaml` `feature_engineering` section; set `enabled: false` to disable entirely
- Validated via Pydantic: lags must be positive integers, rolling windows must be >= 2

### Documentation
- Updated `README.md` with time-series feature engineering in feature list
- Updated `config.yaml` with documented `feature_engineering` section
- Added `feature_engineering` module to `setup.py` py_modules list

## [1.24.0] - 2026-02-09

### Added
- **Production Model Configuration**: Explicit production default model selection via `config.yaml`. New `production.default_model` field allows easy switching between ML models without code changes.
- **Enhanced Model Comparison Report**: Comprehensive reporting system for model evaluation and selection:
  - **CSV Report** (`reports/model_comparison_report.csv`): Consolidated metrics table with all ML and baseline models ranked by Test MAE and Test RMSE
  - **Markdown Summary** (`reports/model_comparison_summary.md`): Decision-ready report with:
    - Recommended production model with performance metrics
    - Detailed rationale for model selection
    - Top 3 models comparison table
    - Complete rankings of all models
    - Warning flags for models with negative R² scores
    - Instructions for switching production models
- **Negative R² Detection**: Automatic flagging of models with negative R² scores in both CSV (`R2 Warning` column) and Markdown reports (⚠️ emoji marker)
- **Consistent Report Archiving**: All comparison artifacts saved in `reports/` subdirectory with standardized naming convention
- New `ProductionConfig` validation class in `config.py` ensuring only valid model names can be set as production default
- UTF-8 encoding support for markdown files to properly render emoji symbols on Windows

### Changed
- `write_comparison_report()` in `baselines.py` now generates both CSV and Markdown outputs
- Model comparison reports include both ML models and baseline forecasters for comprehensive evaluation
- Report sorting prioritizes Test MAE, then Test RMSE for model ranking
- Version bump 1.23.0 → 1.24.0 (minor) for new production model configuration and enhanced reporting features

### Technical Details
- Production model validation accepts: "Linear Regression", "Decision Tree", "Random Forest", "Gradient Boosting"
- Markdown report uses UTF-8 encoding to support emoji symbols cross-platform
- R² warning flags applied when R² < 0 (predictions worse than mean baseline)
- Report generation integrated into existing model evaluation pipeline (no breaking changes)
- Config validation via Pydantic ensures type safety for production model selection

### Documentation
- Updated `config.yaml` with new `production` section and inline documentation
- Enhanced `README.md` with production model configuration usage examples


## [1.23.0] - 2026-02-08

### Added
- **Target Transformation**: Optional log-based transformation for target variable to handle skewed expense distributions. Configurable via `target_transform` section in config.yaml with `enabled` (boolean) and `method` (log1p or log) fields. Predictions are automatically transformed back to original scale.
- **Robust Evaluation Metrics**: New metrics less sensitive to outliers and better suited for expense prediction evaluation:
  - Median Absolute Error (MedAE) - robust alternative to MAE
  - Symmetric Mean Absolute Percentage Error (SMAPE) - percentage-based metric handling near-zero values
  - Percentile-based error distribution (P50, P75, P90) - highlights worst-case prediction errors
- Helper functions in helpers.py: `apply_target_transform()`, `inverse_target_transform()`, `calculate_median_absolute_error()`, `calculate_smape()`, `calculate_percentile_errors()`
- Comprehensive test suite: 15 new tests for transformation and metrics (8 transformation tests, 7 metrics tests)
- Enhanced logging: All evaluation outputs now include both standard metrics (RMSE, MAE, R²) and robust metrics with clear labeling

### Changed
- Model evaluation now reports extended metrics in logs, making it easier to understand prediction quality and error distribution
- Version bump 1.22.0 → 1.23.0 (minor) for new target transformation and robust evaluation capabilities

### Technical Details
- Transformations use numpy's log1p/expm1 for numerical stability with zeros and small values
- All metrics computed on original scale even when transformation is enabled
- Transformation is fully backward compatible (disabled by default)
- SMAPE handles division by zero when both actual and predicted are zero


## [1.22.0] - 2026-02-08

### Added
- Baseline forecasters (naive last value, 3-month and 6-month rolling means, and seasonal naive when ≥12 months of history) with prediction outputs in the same schema/location as existing models.
- Baseline evaluation metrics (RMSE, MAE, R²) logged for train/test splits alongside ML models.
- Model comparison report (`model_comparison_report.csv`) ranking ML models and baselines by test MAE and RMSE.
- Config/CLI toggles for baselines (`baselines.enabled` in config.yaml and `--skip_baselines` flag).

### Changed
- Version bump 1.21.0 → 1.22.0 (minor) for new baseline forecasting and reporting capabilities.

### Fixed
- Wrote the model comparison report to `output_dir/reports/` to avoid interfering with prediction CSV validation in CLI tests.
- Simplified baseline runner structure and centralised report column labels.

## [1.21.0] - 2026-02-08

### Fixed
- **Time-Series Train/Test Leakage**: Replaced sklearn `train_test_split` with explicit chronological splitting to prevent data leakage in time-dependent expense data. New `chronological_train_test_split` function enforces strict temporal ordering: first 80% of data for training, last 20% for testing, with validation that dates are strictly increasing (no duplicates). Train/test date boundaries are now logged explicitly for transparency.

### Added
- `chronological_train_test_split()` function in helpers.py for time-aware data splitting with date boundary logging, chronological order validation, and duplicate date detection.
- Unit tests for chronological splitting (`TestChronologicalTrainTestSplit`: 8 tests) covering split ratio, temporal ordering, unsorted data rejection, duplicate date rejection, and various test sizes.
- Leakage detection tests (`TestFutureDataLeakage`: 5 tests) verifying no future targets in training, features are date-intrinsic, strict date boundaries, no shuffling, and full pipeline integration.

### Impact
- **Severity**: Critical | **Type**: Bug Fix | **Breaking Changes**: None (evaluation behavior improved)
- Default evaluation split is now strictly chronological. Models never access future target values or features during evaluation. Negative test R² values from inappropriate splitting are resolved.

### Technical Details
- Removed `sklearn.model_selection.train_test_split` dependency from model_runner.py. Split index calculated as `n_samples - int(n_samples * test_size)`. Upfront validation rejects non-monotonic or duplicate date sequences with `DataValidationError`, replacing the previous unreliable post-split overlap check. The upstream pipeline guarantees one row per date via `drop_duplicates` and date-range reindexing; the split function validates this invariant defensively. Version bump 1.20.1 → 1.21.0 (minor) for new function and behavior change.

## [1.20.1] - 2025-11-16

### Changed
- **Condensed and Normalized CHANGELOG** ([#116](https://github.com/manoj-bhaskaran/expense-predictor/issues/116)): Manually condensed CHANGELOG from 109,545 to 13,054 characters (88% reduction) to meet project requirements. All individual entries now ≤400 words. Retained last 5 minor versions (1.16-1.20) as separate entries. Condensed older minors (1.0-1.15) into single summary entry. Improved readability while preserving chronological order and key information.

### Impact
- **Type**: Documentation | **Breaking Changes**: None
- CHANGELOG now manageable and maintainable. Easier to navigate recent changes while preserving historical context in condensed form.

## [1.20.0] - 2025-11-16

### Added
- **Comprehensive Test Coverage Audit** ([#122](https://github.com/manoj-bhaskaran/expense-predictor/issues/122)): Conducted full coverage audit achieving 87.85% coverage (571/650 statements). Generated comprehensive HTML and XML reports. Created `.coveragerc-audit` configuration for periodic audits. Added detailed audit documentation (`coverage-audit/COVERAGE_AUDIT_SUMMARY.md`) breaking down coverage by file and categorizing 79 missing statements by type (error handling, edge cases, validation, security). Verified minimal and appropriate exclusion rules.

### Documentation
- Added "Coverage Auditing" section to CONTRIBUTING.md with comprehensive guidelines for running audits, interpreting reports, managing exclusions, and testing error paths. Documented quarterly review process and best practices for security code testing.

### Impact
- **Severity**: Medium | **Type**: Documentation/Tooling/QA | **Breaking Changes**: None
- **Benefits**: Transparency into actual coverage, prioritized testing roadmap, prevents coverage exclusions from hiding untested code, highlighted security code needing better tests

### Technical Details
- 204 tests passing, 11.97s execution time. Identified critical follow-up: security path validation tests (CRITICAL), error handling tests (HIGH), Excel edge cases (MEDIUM), user interaction tests (LOW). Version bump justified as minor (1.19.0 → 1.20.0) for new development tooling and substantial documentation improvements.

## [1.19.0] - 2025-11-16

### Added
- **Pydantic Type Validation for config.yaml** ([#112](https://github.com/manoj-bhaskaran/expense-predictor/issues/112)): Implemented strict type checking for all configuration parameters to prevent runtime errors. Created validation models for LoggingConfig (log levels), DataProcessingConfig (skiprows ≥0), ModelEvaluationConfig (test_size 0.0-1.0, random_state ≥0), and all ML model hyperparameters (DecisionTree, RandomForest, GradientBoosting). Added 13 comprehensive validation tests covering invalid types, out-of-range values, invalid enums, and multiple errors.

### Impact
- **Severity**: High | **Type**: Feature/Enhancement | **Breaking Changes**: None
- **Benefits**: Fail-fast at startup with clear error messages instead of cryptic runtime errors (e.g., "expected integer, got 'five'" vs "TypeError: '<=' not supported"). Self-documenting schemas, type safety with strict mode, range validation, better debugging for non-technical users.

### Technical Details
- Files modified: `config.py` (Pydantic models), `requirements.txt` (pydantic==2.10.3), `tests/test_config.py` (13 validation tests). All 27 config tests pass. Version bump (1.18.3 → 1.19.0) justified as minor for new type validation capability.

## [1.18.0 → 1.18.3] - 2025-11-16

### Summary
Four patch releases focused on data validation, code quality improvements, and dependency management.

### Added
- **Minimum Data Validation** ([#108](https://github.com/manoj-bhaskaran/expense-predictor/issues/108)): validate_minimum_data() function validating sufficient training data (min 30 total, 10 test samples). Configurable thresholds via config.yaml. Prevents crashes with clear error messages. Added "Data Requirements" section to README.

### Changed
- **Deprecated pandas API** ([#111](https://github.com/manoj-bhaskaran/expense-predictor/issues/111)): Removed all inplace=True uses, replaced with method chaining. Future-proof for pandas 3.0+.
- **Constants Module** ([#110](https://github.com/manoj-bhaskaran/expense-predictor/issues/110)): Created constants.py for centralized definitions. Moved TRANSACTION_AMOUNT_LABEL, VALUE_DATE_LABEL, DAY_OF_WEEK. Single source of truth.

### Fixed
- **line-profiler Version** ([#109](https://github.com/manoj-bhaskaran/expense-predictor/issues/109)): Updated to 4.1.3 in setup.py, matching requirements-dev.txt. Ensures consistent dev environments.

### Impact
Critical data validation enhancement. High-impact code quality improvements. All changes backward compatible.

## [1.17.0 → 1.17.2] - (2025-11-15 → 2025-11-16)

### Summary
Three patch releases adding Python 3.12 support and fixing critical Excel processing issues.

### Added
- **Python 3.12 Support** ([#87](https://github.com/manoj-bhaskaran/expense-predictor/issues/87)): Added to CI/CD test matrix. All 163 tests pass on Python 3.9-3.12. Updated line-profiler to 4.1.3 for Python 3.12 compatibility. Updated README.

### Fixed
- **Missing Production Dependency** ([#106](https://github.com/manoj-bhaskaran/expense-predictor/issues/106)): Added openpyxl==3.1.2 to requirements.txt and setup.py. Fixes ModuleNotFoundError for .xlsx processing in production.
- **Excel Error Handling** ([#107](https://github.com/manoj-bhaskaran/expense-predictor/issues/107)): Added explicit ImportError handling with installation instructions for missing openpyxl.

### Documentation
- Added "Excel File Support" section to README clarifying .xls (xlrd) vs .xlsx (openpyxl) distinction with format detection and usage examples.

### Impact
Python 3.12 support (new feature). Critical dependency fix for .xlsx files. Improved error messages for Excel processing.

## [1.16.0] - 2025-11-16

### Added
- **Configurable Log Levels**: Added `--log-level` CLI argument with choices (DEBUG, INFO, WARNING, ERROR, CRITICAL), `EXPENSE_PREDICTOR_LOG_LEVEL` environment variable support, and logging configuration in config.yaml. Priority order: CLI > environment > config > default (INFO). Automatic validation with graceful fallback to INFO for invalid values.

### Impact
- **Type**: Feature | **Breaking Changes**: None
- Enhanced logging flexibility for development, production, and CI/CD. Improved debugging capabilities with configurable verbosity.

## [1.0.0 → 1.15.0] - (2025-11-14 → 2025-11-15)

### Summary
Foundational development from initial release through comprehensive testing, security, CI/CD infrastructure, and developer experience improvements.

### Major Additions
- **1.15.0**: Pre-commit hooks (.pre-commit-config.yaml) with black, isort, flake8, mypy, bandit, yamllint for automatic code quality checks before commits.
- **1.14.0**: Environment variable configuration via .env files (python-dotenv). Variables: DATA_FILE, EXCEL_DIR, EXCEL_FILE, LOG_DIR, OUTPUT_DIR, FUTURE_DATE, SKIP_CONFIRMATION. Priority: CLI > env > config > defaults.
- **1.13.0**: Pytest markers (@pytest.mark.unit, .integration, .slow, .validation) for selective test execution. Staged CI/CD testing for faster feedback.
- **1.12.0-1.12.1**: Logging framework tests (22 tests, 100% coverage for python_logging_framework.py). Enhanced documentation.
- **1.11.0-1.11.2**: Enhanced data processing with normalization, flexible column detection, feature engineering. Extensive unit tests.
- **1.10.0-1.10.1**: Improved Excel integration with robust column matching and error handling for varying bank statement formats.
- **1.9.0**: Removed external GitHub dependency, made local python_logging_framework official.
- **1.8.0-1.8.1**: Enhanced model evaluation metrics with separate train/test reporting and improved logging format.
- **1.7.0**: CI/CD pipeline (test.yml) with multi-Python version testing and 80% coverage enforcement.
- **1.6.0**: Security scanning workflow (Bandit, Safety) with scheduled weekly scans and automated vulnerability detection.
- **1.5.0**: Testing framework (pytest/pytest-cov/pytest-mock). 57 initial tests, 43% coverage. Test structure with conftest.py, test data, fixtures.
- **1.4.0**: Security module (security.py) with path validation, CSV injection prevention, backups, user confirmations, --skip_confirmation flag.
- **1.3.0-1.3.1**: Standardized logging (replaced print() with plog), comprehensive pipeline logging, optimized categorical handling.
- **1.2.0**: Input validation (validate_csv_file, validate_excel_file, validate_date_range) for early error detection with clear messages.
- **1.1.0**: Configuration system (config.yaml, config.py) centralizing parameters (data processing, model hyperparameters). Graceful defaults.
- **1.0.0-1.0.5**: Initial release with four ML models (Linear Regression, Decision Tree, Random Forest, Gradient Boosting), CSV/Excel processing, feature engineering, CLI, logging, metrics (RMSE, MAE, R²). Fixes for dependencies, train/test split, column renaming, data mutation.

### Infrastructure & Quality
Complete CI/CD pipeline (testing, pre-commit, security, Dependabot). Test coverage: 43% (v1.5.0) to 89%+ (v1.12.1+). Security: multi-layer scanning, path/CSV injection protection. Developer experience: pre-commit hooks, selective tests, env vars, comprehensive docs.

### Breaking Changes
None - all releases backward compatible

### Technical Stack
Dependencies: pytest, pytest-cov, pytest-mock, pydantic, python-dotenv, pre-commit, line-profiler, bandit. Python 3.9-3.12. Core: scikit-learn, pandas, numpy.

---

## How to Report Issues

If you encounter bugs or have feature requests, please open an issue on the [GitHub repository](https://github.com/manoj-bhaskaran/expense-predictor/issues).

Include: version number, OS and Python version, complete error messages, reproduction steps, anonymized sample data if relevant.

---

[Unreleased]: https://github.com/manoj-bhaskaran/expense-predictor/compare/v1.22.1...HEAD
[1.22.1]: https://github.com/manoj-bhaskaran/expense-predictor/compare/v1.22.0...v1.22.1
[1.22.0]: https://github.com/manoj-bhaskaran/expense-predictor/compare/v1.21.0...v1.22.0
[1.21.0]: https://github.com/manoj-bhaskaran/expense-predictor/compare/v1.20.1...v1.21.0
[1.20.1]: https://github.com/manoj-bhaskaran/expense-predictor/releases/tag/v1.20.1
[1.20.0]: https://github.com/manoj-bhaskaran/expense-predictor/releases/tag/v1.20.0
[1.19.0]: https://github.com/manoj-bhaskaran/expense-predictor/releases/tag/v1.19.0
[1.18.0 → 1.18.3]: https://github.com/manoj-bhaskaran/expense-predictor/compare/v1.18.0...v1.18.3
[1.17.0 → 1.17.2]: https://github.com/manoj-bhaskaran/expense-predictor/compare/v1.17.0...v1.17.2
[1.16.0]: https://github.com/manoj-bhaskaran/expense-predictor/releases/tag/v1.16.0
[1.0.0 → 1.15.0]: https://github.com/manoj-bhaskaran/expense-predictor/compare/v1.0.0...v1.15.0
